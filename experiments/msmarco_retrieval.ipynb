{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS MARCO Retrieval Pipeline with Permutation Self-Consistency\n",
    "\n",
    "This notebook demonstrates the full retrieval pipeline:\n",
    "1. Initial retrieval using BM25 or SPLADE++\n",
    "2. Optional LLM reranking with permutation self-consistency\n",
    "3. Evaluation on TREC DL19/20 datasets using NDCG and MRR metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version: openjdk version \"11.0.29\" 2025-10-21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set JAVA_HOME for Pyserini (required for BM25 retrieval)\n",
    "# This ensures Java 11+ is used even if Jupyter doesn't inherit shell environment\n",
    "if 'JAVA_HOME' not in os.environ or '1.8' in os.popen('java -version 2>&1').read():\n",
    "    # Try to find Java 11+ via Homebrew\n",
    "    java_home_candidates = [\n",
    "        '/usr/local/Cellar/openjdk@11/11.0.29/libexec/openjdk.jdk/Contents/Home',\n",
    "        '/usr/local/Cellar/openjdk@17/17.0.13/libexec/openjdk.jdk/Contents/Home',\n",
    "        '/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home',\n",
    "        '/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home',\n",
    "    ]\n",
    "    \n",
    "    for candidate in java_home_candidates:\n",
    "        if os.path.exists(candidate):\n",
    "            os.environ['JAVA_HOME'] = candidate\n",
    "            os.environ['PATH'] = f\"{candidate}/bin:{os.environ.get('PATH', '')}\"\n",
    "            print(f\"Set JAVA_HOME to: {candidate}\")\n",
    "            break\n",
    "    else:\n",
    "        # Fallback: try to use java_home utility\n",
    "        try:\n",
    "            import subprocess\n",
    "            java_home = subprocess.check_output(['/usr/libexec/java_home', '-v', '11+']).decode().strip()\n",
    "            os.environ['JAVA_HOME'] = java_home\n",
    "            os.environ['PATH'] = f\"{java_home}/bin:{os.environ.get('PATH', '')}\"\n",
    "            print(f\"Set JAVA_HOME to: {java_home}\")\n",
    "        except:\n",
    "            print(\"Warning: Could not set JAVA_HOME automatically. Pyserini may not work.\")\n",
    "            print(\"Please ensure Java 11+ is installed and JAVA_HOME is set correctly.\")\n",
    "\n",
    "# Verify Java version\n",
    "if 'JAVA_HOME' in os.environ:\n",
    "    java_version = os.popen(f\"{os.environ['JAVA_HOME']}/bin/java -version 2>&1\").read()\n",
    "    print(f\"Java version: {java_version.split(chr(10))[0]}\")\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from permsc.retrieval import (\n",
    "    MSMarcoQueries, MSMarcoCollection, TRECQrels,\n",
    "    BM25Retriever, SpladeRetriever, RetrievalPipeline,\n",
    "    evaluate_retrieval\n",
    ")\n",
    "from permsc.llm.openai_pool import OpenAIConfig\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and configuration. Update these paths to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: ../data\n",
      "Using prebuilt index: msmarco-v1-passage (will download if needed)\n",
      "OpenAI API key set: False\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "MSMARCO_COLLECTION = DATA_DIR / \"msmarco/collection.tsv\"\n",
    "TREC_DL19_QUERIES = DATA_DIR / \"trec-dl19/msmarco-test2019-queries.tsv\"\n",
    "TREC_DL19_QRELS = DATA_DIR / \"trec-dl19/2019qrels-pass.txt\"\n",
    "TREC_DL20_QUERIES = DATA_DIR / \"trec-dl20/msmarco-test2020-queries.tsv\"\n",
    "TREC_DL20_QRELS = DATA_DIR / \"trec-dl20/2020qrels-pass.txt\"\n",
    "\n",
    "# BM25 Index Configuration\n",
    "# Option 1: Use prebuilt index (downloads automatically - recommended)\n",
    "USE_PREBUILT_INDEX = True\n",
    "PREBUILT_INDEX_NAME = \"msmarco-v1-passage\"\n",
    "\n",
    "# Option 2: Use local index path (if you have downloaded it manually)\n",
    "# USE_PREBUILT_INDEX = False\n",
    "# BM25_INDEX_PATH = \"indexes/msmarco-passage/lucene-index.msmarco-v1-passage.20221004.252b5e\"\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "if USE_PREBUILT_INDEX:\n",
    "    print(f\"Using prebuilt index: {PREBUILT_INDEX_NAME} (will download if needed)\")\n",
    "else:\n",
    "    print(f\"BM25 index path: {BM25_INDEX_PATH}\")\n",
    "print(f\"OpenAI API key set: {bool(OPENAI_API_KEY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO collection...\n",
      "Collection loaded: 8841823 passages\n",
      "\n",
      "Loading TREC DL19 queries and qrels...\n",
      "DL19: 200 queries, 43 queries with qrels\n",
      "\n",
      "Loading TREC DL20 queries and qrels...\n",
      "DL20: 200 queries, 54 queries with qrels\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading MS MARCO collection...\")\n",
    "collection = MSMarcoCollection(str(MSMARCO_COLLECTION))\n",
    "print(f\"Collection loaded: {len(collection)} passages\")\n",
    "\n",
    "print(\"\\nLoading TREC DL19 queries and qrels...\")\n",
    "dl19_queries = MSMarcoQueries(str(TREC_DL19_QUERIES))\n",
    "dl19_qrels = TRECQrels(str(TREC_DL19_QRELS))\n",
    "print(f\"DL19: {len(dl19_queries)} queries, {len(dl19_qrels)} queries with qrels\")\n",
    "\n",
    "print(\"\\nLoading TREC DL20 queries and qrels...\")\n",
    "dl20_queries = MSMarcoQueries(str(TREC_DL20_QUERIES))\n",
    "dl20_qrels = TRECQrels(str(TREC_DL20_QRELS))\n",
    "print(f\"DL20: {len(dl20_queries)} queries, {len(dl20_qrels)} queries with qrels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Retrievers\n",
    "\n",
    "Choose which retriever to use: BM25 or SPLADE++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BM25 retriever with prebuilt index: msmarco-v1-passage\n",
      "Retriever initialized successfully\n"
     ]
    }
   ],
   "source": [
    "RETRIEVER_TYPE = \"bm25\"  # or \"splade\"\n",
    "\n",
    "if RETRIEVER_TYPE == \"bm25\":\n",
    "    if USE_PREBUILT_INDEX:\n",
    "        print(f\"Initializing BM25 retriever with prebuilt index: {PREBUILT_INDEX_NAME}\")\n",
    "        retriever = BM25Retriever(prebuilt_index=PREBUILT_INDEX_NAME)\n",
    "    else:\n",
    "        print(f\"Initializing BM25 retriever with local index: {BM25_INDEX_PATH}\")\n",
    "        retriever = BM25Retriever(index_path=BM25_INDEX_PATH)\n",
    "elif RETRIEVER_TYPE == \"splade\":\n",
    "    print(\"Initializing SPLADE++ retriever...\")\n",
    "    retriever = SpladeRetriever(str(MSMARCO_COLLECTION))\n",
    "else:\n",
    "    raise ValueError(f\"Unknown retriever type: {RETRIEVER_TYPE}\")\n",
    "\n",
    "print(\"Retriever initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM Reranking (Optional)\n",
    "\n",
    "If API key is provided, LLM reranking with permutation self-consistency will be enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No API key provided. LLM reranking disabled.\n",
      "No LLM config provided or API key missing. LLM reranking will be disabled.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "llm_config = None\n",
    "if OPENAI_API_KEY:\n",
    "    llm_config = OpenAIConfig(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        api_type=\"openai\"\n",
    "    )\n",
    "    print(\"LLM reranking enabled\")\n",
    "else:\n",
    "    print(\"No API key provided. LLM reranking disabled.\")\n",
    "\n",
    "pipeline = RetrievalPipeline(\n",
    "    retriever=retriever,\n",
    "    collection=collection,\n",
    "    llm_config=llm_config,\n",
    "    num_permutations=5,\n",
    "    aggregator=\"kemeny\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Retrieval and Evaluation\n",
    "\n",
    "Run the pipeline on TREC DL19/20 queries and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running retrieval on DL19 (50 queries)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def run_evaluation(queries, qrels, dataset_name, max_queries=None):\n",
    "    \"\"\"Run retrieval pipeline and evaluate on a dataset.\"\"\"\n",
    "    query_ids = list(queries.get_all_queries().keys())\n",
    "    if max_queries:\n",
    "        query_ids = query_ids[:max_queries]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\nRunning retrieval on {dataset_name} ({len(query_ids)} queries)...\")\n",
    "    for query_id in tqdm(query_ids):\n",
    "        query_text = queries.get_query(query_id)\n",
    "        if not query_text:\n",
    "            continue\n",
    "        \n",
    "        ranking_example = pipeline.run(query_text, top_k=1000, rerank_depth=100)\n",
    "        results[query_id] = ranking_example\n",
    "    \n",
    "    print(f\"\\nEvaluating {dataset_name}...\")\n",
    "    metrics = evaluate_retrieval(results, qrels.get_all_qrels(), k_values=[10, 100])\n",
    "    \n",
    "    return metrics, results\n",
    "\n",
    "metrics_dl19, results_dl19 = run_evaluation(dl19_queries, dl19_qrels, \"DL19\", max_queries=50)\n",
    "metrics_dl20, results_dl20 = run_evaluation(dl20_queries, dl20_qrels, \"DL20\", max_queries=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Dataset': ['DL19', 'DL20'],\n",
    "    'NDCG@10': [metrics_dl19['ndcg@10'], metrics_dl20['ndcg@10']],\n",
    "    'NDCG@100': [metrics_dl19['ndcg@100'], metrics_dl20['ndcg@100']],\n",
    "    'MRR': [metrics_dl19['mrr'], metrics_dl20['mrr']]\n",
    "})\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nRetriever: {RETRIEVER_TYPE.upper()}\")\n",
    "print(f\"LLM Reranking: {'Enabled' if llm_config else 'Disabled'}\")\n",
    "if llm_config:\n",
    "    print(f\"Permutations: {pipeline.num_permutations}\")\n",
    "    print(f\"Aggregator: {pipeline.aggregator_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics_to_plot = ['NDCG@10', 'NDCG@100', 'MRR']\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    values = results_df[metric].values\n",
    "    ax.bar(results_df['Dataset'], values, color=['#3498db', '#e74c3c'])\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'{metric} by Dataset')\n",
    "    ax.set_ylim(0, max(values) * 1.2)\n",
    "    \n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(i, v + max(values) * 0.02, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Configurations\n",
    "\n",
    "To compare BM25 vs SPLADE++ or with/without LLM reranking, run the cells above with different configurations and compare the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
